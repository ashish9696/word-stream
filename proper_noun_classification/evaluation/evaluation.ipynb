{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import data_utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1479175091\n",
    "#1479582035\n",
    "#1479176649\n",
    "checkpoint_dir = os.path.join(os.path.abspath(os.path.curdir), \"runs\", \"1512866911\", \"checkpoints\")\n",
    "\n",
    "# Shared Parameters\n",
    "# ==================================================\n",
    "embedding_dim = 64\n",
    "batch_size = 54\n",
    "hidden_size = 100\n",
    "num_classes = 5\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{';': 119, 'ü': 75, '-': 34, 'ñ': 89, 'ä': 74, 'R': 37, 'S': 19, '$': 120, 'F': 35, ':': 53, '&': 63, 'U': 55, 'ý': 140, ')': 78, 'Ô': 112, '`': 143, 'ã': 90, 'ì': 124, 'P': 25, 'O': 46, 'Â': 123, 'ö': 72, 'J': 45, 'Ð': 136, 'C': 17, '[': 142, 'Ë': 111, 'd': 13, 'Ü': 121, 'L': 29, 'á': 79, 'y': 20, 'D': 27, 'Ä': 126, 'Q': 69, 'æ': 113, 'w': 40, '0': 50, 'k': 31, 'Ñ': 118, 'G': 38, 'K': 48, '7': 71, '÷': 138, 'g': 16, 'ð': 109, '~': 0, 'õ': 107, 'Ø': 125, 'Ö': 122, 'à': 85, 'þ': 141, '°': 133, '@': 145, 'û': 97, '6': 66, 'ï': 128, 'Ó': 101, 'Ì': 132, 'Z': 64, 'ó': 80, '¿': 129, 'l': 9, 'h': 14, 'Ç': 110, 'é': 62, 'É': 95, 'V': 49, 's': 10, '(': 76, 'â': 92, 'E': 39, 'x': 43, 'T': 21, 'Õ': 115, '2': 51, 'N': 41, 'è': 77, '3': 56, '5': 58, ']': 137, 'í': 81, 'f': 30, 'ê': 102, '_': 139, 'c': 12, '\"': 100, '*': 98, 'A': 22, 'ß': 96, '4': 57, 'b': 28, 'å': 84, 'I': 24, 't': 8, 'ÿ': 130, 'Ò': 103, 'Ê': 131, 'q': 59, 'r': 6, '9': 65, 'o': 5, '/': 70, 'î': 116, 'X': 67, 'n': 4, 'ú': 108, 'j': 60, 'v': 33, 'u': 11, 'Å': 93, 'm': 15, 'i': 7, '1': 44, ',': 54, 'ô': 86, 'p': 18, '?': 82, '#': 88, 'ë': 114, 'Ù': 135, 'Y': 61, '!': 73, '¼': 146, 'z': 47, 'Î': 99, 'ç': 83, '8': 68, '^': 1, 'Á': 105, '×': 134, 'ù': 144, '+': 94, 'W': 42, 'B': 26, 'a': 3, \"'\": 52, 'H': 36, '.': 32, 'M': 23, 'Í': 117, 'Ï': 106, '%': 91, 'ø': 87, 'À': 104, '¡': 127, 'e': 2}\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "# ==================================================\n",
    "test_filename = \"pnp-test.txt\"\n",
    "\n",
    "x_test, y_test, seq_lens_test, vocab_dict \\\n",
    "    = data_utils.load_data_and_labels(test_filename, train=False)\n",
    "test_data = list(zip(x_test, y_test, seq_lens_test))\n",
    "\n",
    "print(vocab_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/nbuser/library/runs/1512866911/checkpoints/model-4001\n",
      "2357\n",
      "Total number of test examples: 2862\n",
      "Accuracy: 0.82355\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "# ==================================================\n",
    "checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    sess = tf.Session()\n",
    "\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        seq_lens = graph.get_operation_by_name(\"seq_lens\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"prediction/predictions\").outputs[0]\n",
    "        prediction_probs = graph.get_operation_by_name(\"prediction/prediction_probs\").outputs[0]\n",
    "        correct_vector = graph.get_operation_by_name(\"prediction/correct_vector\").outputs[0]\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = data_utils.batch_iter(test_data, batch_size=batch_size, num_epochs=1, shuffle=False)\n",
    "\n",
    "        probs = []\n",
    "\n",
    "        for batch in batches:\n",
    "            x_batch = batch[:,0]\n",
    "            y_batch = batch[:,1]\n",
    "            seq_lens_batch = batch[:,2]\n",
    "\n",
    "            x_batch = data_utils.pad(x_batch, seq_lens_batch)\n",
    "\n",
    "            batch_predictions, correct, batch_probs = sess.run([predictions, correct_vector, prediction_probs], \\\n",
    "            #batch_predictions = sess.run([predictions], \\\n",
    "                {input_x: x_batch, input_y: y_batch, seq_lens: seq_lens_batch})\n",
    "            #print(batch_probs)\n",
    "            correct += np.sum(correct)\n",
    "            #print(y_batch)\n",
    "            all_labels = np.concatenate([all_labels, y_batch]) #np.concatenate([all_labels, [np.argmax(x) for x in y_batch]])\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "            for i in range(len(y_batch)):\n",
    "                probs.append(batch_probs[i][y_batch[i]])\n",
    "\n",
    "# Print accuracy if y_test is defined; [np.argmax(x) for x in all_labels]\n",
    "correct_predictions = np.sum(all_predictions == np.array(all_labels))\n",
    "print(correct_predictions)\n",
    "print(\"Total number of test examples: {}\".format(len(seq_lens_test)))\n",
    "print(\"Accuracy: {:g}\".format(correct_predictions/float(len(seq_lens_test))))\n",
    "\n",
    "labels = [\"drug\", \"person\", \"place\", \"movie\", \"company\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  4.,  4.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write predictions\n",
    "test_filename = \"pnp-test.txt\"\n",
    "\n",
    "file_path = os.path.join(os.path.abspath(os.path.curdir), test_filename)\n",
    "f = open(file_path, 'r', encoding = \"ISO-8859-1\")\n",
    "data = list(f.readlines())\n",
    "f.close()\n",
    "data = [s.strip().split() for s in data]\n",
    "\n",
    "proper_nouns_strings = [\" \".join(d[1:]) for d in data]\n",
    "\n",
    "f = open(\"output2.txt\", 'w', encoding = \"ISO-8859-1\")\n",
    "for i in range(len(proper_nouns_strings)):\n",
    "    f.write(\"Example: \"+proper_nouns_strings[i]+\" guess=\"+str(labels[int(all_predictions[i])])+\" gold= confidence=\"+str(probs[i])+\"\\n\")\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
