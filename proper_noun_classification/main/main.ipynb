{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tf_helpers\n",
    "import data_utils\n",
    "from RNNClassification import RNNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code based on: https://github.com/dennybritz/cnn-text-classification-tf\n",
    "\n",
    "embedding_dim = 100\n",
    "batch_size = 54\n",
    "hidden_size = 100\n",
    "num_classes = 5\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "# =================================================================================\n",
    "train_filename = \"pnp-train.txt\"\n",
    "validate_filename = \"pnp-validate.txt\"\n",
    "test_filename = \"pnp-test.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train, seq_lens_train, vocab_dict \\\n",
    "    = data_utils.load_data_and_labels(train_filename)\n",
    "train_data = list(zip(x_train, y_train, seq_lens_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_dev, y_dev, seq_lens_dev, vocab_dict \\\n",
    "    = data_utils.load_data_and_labels(validate_filename, train=False)\n",
    "dev_data = list(zip(x_dev, y_dev, seq_lens_dev))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "# =================================================================================\n",
    "def train_step(x_batch, y_batch, seq_lens, current_step, writer=None, print_bool=False):\n",
    "    \"\"\"\n",
    "    Single training step\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "        nn.input_x: x_batch,\n",
    "        nn.input_y: y_batch,\n",
    "        nn.seq_lens: seq_lens\n",
    "    }\n",
    "    _, summaries, loss_val, accuracy_val = sess.run([train_op, train_summary_op, nn.loss, nn.accuracy], feed_dict)\n",
    "\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    if print_bool:\n",
    "        print(\"\\nTrain: {}: step {}, loss {:g}, acc {:g}\".format(time_str, current_step, loss_val, accuracy_val))\n",
    "    if writer:\n",
    "        writer.add_summary(summaries, current_step)\n",
    "\n",
    "    return (loss_val, accuracy_val)\n",
    "\n",
    "def dev_eval(dev_data, current_step, writer=None):\n",
    "    \"\"\"\n",
    "    Evaluates model on a validation set\n",
    "    \"\"\"\n",
    "    batches = data_utils.batch_iter(dev_data, batch_size=batch_size, num_epochs=1, shuffle=False)\n",
    "    loss_val = 0\n",
    "    accuracy_val = 0\n",
    "\n",
    "    batch_count = 0\n",
    "    for dev_batch in batches:\n",
    "        batch_count += 1\n",
    "\n",
    "        x_batch = dev_batch[:,0]\n",
    "        y_batch = dev_batch[:,1]\n",
    "        seq_lens_batch = dev_batch[:,2]\n",
    "\n",
    "        x_batch = data_utils.pad(x_batch, seq_lens_batch)\n",
    "\n",
    "        loss_val_batch, accuracy_val_batch = dev_step(x_batch, y_batch, seq_lens_batch, \\\n",
    "            current_step, writer=writer)\n",
    "        loss_val += loss_val_batch\n",
    "        accuracy_val += accuracy_val_batch\n",
    "\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    print(\"Dev:   {}: step {}, loss {:g}, acc {:g}\".format(time_str, current_step, \\\n",
    "        loss_val/batch_count, accuracy_val/batch_count))\n",
    "\n",
    "    return (loss_val, accuracy_val)\n",
    "\n",
    "def dev_step(x_batch, y_batch, seq_lens, current_step, writer=None):\n",
    "    \"\"\"\n",
    "    Evaluates model on a validation set\n",
    "    \"\"\"\n",
    "\n",
    "    x_batch_padded = data_utils.pad(x_batch, seq_lens)\n",
    "\n",
    "    feed_dict = {\n",
    "        nn.input_x: x_batch_padded,\n",
    "        nn.input_y: y_batch,\n",
    "        nn.seq_lens: seq_lens\n",
    "    }\n",
    "    summaries, loss_val, accuracy_val = sess.run([dev_summary_op, nn.loss, nn.accuracy], feed_dict)\n",
    "    if writer:\n",
    "        writer.add_summary(summaries, current_step)\n",
    "\n",
    "    return (loss_val, accuracy_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Starting Session\n",
    "# ================================================================================\n",
    "\n",
    "graph = tf.Graph()\n",
    "sess = tf.InteractiveSession()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"rnn/Slice:0\", shape=(?, ?, ?), dtype=float32)\n",
      "Tensor(\"rnn/Slice:0\", shape=(?, ?, ?), dtype=float32)\n",
      "\n",
      "\n",
      "Tensor(\"rnn/Slice_1:0\", shape=(?, ?, ?), dtype=float32)\n",
      "Tensor(\"rnn/Slice_1:0\", shape=(?, ?, ?), dtype=float32)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn = RNNClassifier(\n",
    "        num_classes=num_classes,\n",
    "        vocab_size=len(vocab_dict),\n",
    "        hidden_size=hidden_size,\n",
    "        embedding_dim=embedding_dim,\n",
    "        batch_size=batch_size,\n",
    "        bidirectional=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate) # TODO: CHOOSE YOUR FAVORITE OPTIMZER\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "grads_and_vars = optimizer.compute_gradients(nn.loss)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W_embeddings:0/grad/hist is illegal; using embedding/W_embeddings_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W_embeddings:0/grad/sparsity is illegal; using embedding/W_embeddings_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name rnn/Cell-Time/GRU-Forward/Gates/reset_weight:0/grad/hist is illegal; using rnn/Cell-Time/GRU-Forward/Gates/reset_weight_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name rnn/Cell-Time/GRU-Forward/Gates/reset_weight:0/grad/sparsity is illegal; using rnn/Cell-Time/GRU-Forward/Gates/reset_weight_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name rnn/Cell-Time/GRU-Forward/Gates/update_weight:0/grad/hist is illegal; using rnn/Cell-Time/GRU-Forward/Gates/update_weight_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name rnn/Cell-Time/GRU-Forward/Gates/update_weight:0/grad/sparsity is illegal; using rnn/Cell-Time/GRU-Forward/Gates/update_weight_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name rnn/Cell-Time/GRU-Forward/Gates/reset_bias:0/grad/hist is illegal; using rnn/Cell-Time/GRU-Forward/Gates/reset_bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name rnn/Cell-Time/GRU-Forward/Gates/reset_bias:0/grad/sparsity is illegal; using rnn/Cell-Time/GRU-Forward/Gates/reset_bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name rnn/Cell-Time/GRU-Forward/Gates/update_bias:0/grad/hist is illegal; using rnn/Cell-Time/GRU-Forward/Gates/update_bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name rnn/Cell-Time/GRU-Forward/Gates/update_bias:0/grad/sparsity is illegal; using rnn/Cell-Time/GRU-Forward/Gates/update_bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name rnn/Cell-Time/GRU-Forward/Candidate/candidate_bias:0/grad/hist is illegal; using rnn/Cell-Time/GRU-Forward/Candidate/candidate_bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name rnn/Cell-Time/GRU-Forward/Candidate/candidate_bias:0/grad/sparsity is illegal; using rnn/Cell-Time/GRU-Forward/Candidate/candidate_bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name rnn/Cell-Time/GRU-Backward/Gates/reset_weight:0/grad/hist is illegal; using rnn/Cell-Time/GRU-Backward/Gates/reset_weight_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name rnn/Cell-Time/GRU-Backward/Gates/reset_weight:0/grad/sparsity is illegal; using rnn/Cell-Time/GRU-Backward/Gates/reset_weight_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name rnn/Cell-Time/GRU-Backward/Gates/update_weight:0/grad/hist is illegal; using rnn/Cell-Time/GRU-Backward/Gates/update_weight_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name rnn/Cell-Time/GRU-Backward/Gates/update_weight:0/grad/sparsity is illegal; using rnn/Cell-Time/GRU-Backward/Gates/update_weight_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name rnn/Cell-Time/GRU-Backward/Gates/reset_bias:0/grad/hist is illegal; using rnn/Cell-Time/GRU-Backward/Gates/reset_bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name rnn/Cell-Time/GRU-Backward/Gates/reset_bias:0/grad/sparsity is illegal; using rnn/Cell-Time/GRU-Backward/Gates/reset_bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name rnn/Cell-Time/GRU-Backward/Gates/update_bias:0/grad/hist is illegal; using rnn/Cell-Time/GRU-Backward/Gates/update_bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name rnn/Cell-Time/GRU-Backward/Gates/update_bias:0/grad/sparsity is illegal; using rnn/Cell-Time/GRU-Backward/Gates/update_bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name rnn/Cell-Time/GRU-Backward/Candidate/candidate_bias:0/grad/hist is illegal; using rnn/Cell-Time/GRU-Backward/Candidate/candidate_bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name rnn/Cell-Time/GRU-Backward/Candidate/candidate_bias:0/grad/sparsity is illegal; using rnn/Cell-Time/GRU-Backward/Candidate/candidate_bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name prediction/predict_weight:0/grad/hist is illegal; using prediction/predict_weight_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name prediction/predict_weight:0/grad/sparsity is illegal; using prediction/predict_weight_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name prediction/predict_bias:0/grad/hist is illegal; using prediction/predict_bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name prediction/predict_bias:0/grad/sparsity is illegal; using prediction/predict_bias_0/grad/sparsity instead.\n",
      "Writing to /home/nbuser/library/runs/1512866911\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-12-8099b7a7288e>:2: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    }
   ],
   "source": [
    "train_summary_op, dev_summary_op, train_summary_writer, dev_summary_writer, timestamp, checkpoint_prefix = \\\n",
    "    tf_helpers.save_summaries(sess, nn.loss, nn.accuracy, grads_and_vars)\n",
    "saver = tf.train.Saver(tf.all_variables())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-0a04c3a3063d>:3: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "# Training and Validation\n",
    "# ===============================================================================\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "\n",
    "def loss_early_stopping():\n",
    "    min_loss = 999999999\n",
    "    increasing_loss_count = 0\n",
    "    max_accuracy = 0\n",
    "    max_accuracy_step = 0\n",
    "\n",
    "    for batch in batches:\n",
    "        x_batch, y_batch = zip(*batch) # TODO: SETUP YOUR DATA'S BATCHES\n",
    "\n",
    "        current_step = tf.train.global_step(sess, global_step)\n",
    "        if current_step % 500 == 0:\n",
    "            train_loss, train_accuracy = train_step(x_batch, y_batch, current_step, print_bool=True)\n",
    "            dev_loss, dev_accuracy = dev_step(x_dev, y_dev, current_step)\n",
    "\n",
    "            if dev_loss < min_loss:\n",
    "                min_loss = dev_loss\n",
    "                increasing_loss_count = 0\n",
    "            else:\n",
    "                increasing_loss_count += 1\n",
    "\n",
    "            if dev_accuracy > max_accuracy:\n",
    "                max_accuracy = dev_accuracy\n",
    "                max_accuracy_step = current_step\n",
    "\n",
    "            if current_step > FLAGS.patience and FLAGS.patience_increase < increasing_loss_count:\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            train_loss, train_accuracy = train_step(x_batch, y_batch, current_step, print_bool=False)\n",
    "\n",
    "        if current_step % FLAGS.checkpoint_every == 0:\n",
    "            path = saver.save(sess, checkpoint_prefix, global_step=global_step)\n",
    "            print(\"Saved model checkpoint to {}\".format(path))\n",
    "\n",
    "    return (train_loss, train_accuracy, max_accuracy, max_accuracy_step)\n",
    "\n",
    "def accuracy_early_stopping():\n",
    "    max_accuracy = 0\n",
    "    max_accuracy_step = 0\n",
    "\n",
    "    #for batch in batches:\n",
    "        #x_batch, y_batch = zip(*batch) # TODO: SETUP YOUR DATA'S BATCHES\n",
    "\n",
    "    for _ in range(1000):\n",
    "        x_batch, y_batch = mnist.train.next_batch(FLAGS.batch_size)\n",
    "\n",
    "        current_step = tf.train.global_step(sess, global_step)\n",
    "        if current_step % FLAGS.evaluate_every == 0:\n",
    "            train_loss, train_accuracy = train_step(x_batch, y_batch, current_step, print_bool=True)\n",
    "            dev_loss, dev_accuracy = dev_step(x_dev, y_dev, current_step)\n",
    "\n",
    "            if dev_accuracy > max_accuracy:\n",
    "                max_accuracy = dev_accuracy\n",
    "                max_accuracy_step = current_step\n",
    "\n",
    "            if current_step > FLAGS.patience and FLAGS.patience_increase < current_step - max_accuracy_step:\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            train_loss, train_accuracy = train_step(x_batch, y_batch, current_step, print_bool=False)\n",
    "\n",
    "        if current_step % FLAGS.checkpoint_every == 0:\n",
    "            path = saver.save(sess, checkpoint_prefix, global_step=global_step)\n",
    "            print(\"Saved model checkpoint to {}\".format(path))\n",
    "\n",
    "    return (train_loss, train_accuracy, max_accuracy, max_accuracy_step)\n",
    "\n",
    "def run_for_epochs(batches):\n",
    "    for batch in batches:\n",
    "        x_batch = batch[:,0]\n",
    "        y_batch = batch[:,1]\n",
    "        seq_lens_batch = batch[:,2]\n",
    "\n",
    "        x_batch = data_utils.pad(x_batch, seq_lens_batch)\n",
    "\n",
    "        current_step = tf.train.global_step(sess, global_step)\n",
    "        if current_step % 100 == 0:\n",
    "            train_loss, train_accuracy = train_step(x_batch, y_batch, seq_lens_batch, \\\n",
    "                current_step, print_bool=True)\n",
    "            dev_loss, dev_accuracy = dev_eval(dev_data, current_step, writer=dev_summary_writer)\n",
    "\n",
    "        else:\n",
    "            train_loss, train_accuracy = train_step(x_batch, y_batch, seq_lens_batch, \\\n",
    "                current_step, print_bool=False)\n",
    "\n",
    "        if current_step % 500 == 0:\n",
    "            path = saver.save(sess, checkpoint_prefix, global_step=global_step)\n",
    "            print(\"Saved model checkpoint to {}\".format(path))\n",
    "\n",
    "    return (train_loss, train_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: 2017-12-10T00:48:33.281347: step 0, loss 87.3176, acc 0.111111\n",
      "Dev:   2017-12-10T00:48:35.395759: step 0, loss 86.6858, acc 0.292103\n",
      "Saved model checkpoint to /home/nbuser/library/runs/1512866911/checkpoints/model-1\n",
      "\n",
      "Train: 2017-12-10T00:48:46.925775: step 100, loss 50.7372, acc 0.62963\n",
      "Dev:   2017-12-10T00:48:49.053673: step 100, loss 47.7146, acc 0.657932\n",
      "\n",
      "Train: 2017-12-10T00:49:00.206269: step 200, loss 50.7139, acc 0.722222\n",
      "Dev:   2017-12-10T00:49:02.282678: step 200, loss 41.1653, acc 0.71174\n",
      "\n",
      "Train: 2017-12-10T00:49:13.282472: step 300, loss 45.1781, acc 0.611111\n",
      "Dev:   2017-12-10T00:49:15.324258: step 300, loss 39.0382, acc 0.720126\n",
      "\n",
      "Train: 2017-12-10T00:49:26.434378: step 400, loss 35.5546, acc 0.740741\n",
      "Dev:   2017-12-10T00:49:28.536949: step 400, loss 40.1909, acc 0.708595\n",
      "\n",
      "Train: 2017-12-10T00:49:39.283592: step 500, loss 27.5175, acc 0.814815\n",
      "Dev:   2017-12-10T00:49:41.482127: step 500, loss 37.1154, acc 0.7355\n",
      "Saved model checkpoint to /home/nbuser/library/runs/1512866911/checkpoints/model-501\n",
      "\n",
      "Train: 2017-12-10T00:49:53.454792: step 600, loss 36.3308, acc 0.722222\n",
      "Dev:   2017-12-10T00:49:55.578652: step 600, loss 36.763, acc 0.740741\n",
      "\n",
      "Train: 2017-12-10T00:50:06.551455: step 700, loss 33.3724, acc 0.759259\n",
      "Dev:   2017-12-10T00:50:08.657008: step 700, loss 34.8067, acc 0.755765\n",
      "\n",
      "Train: 2017-12-10T00:50:19.438571: step 800, loss 31.4117, acc 0.759259\n",
      "Dev:   2017-12-10T00:50:21.819447: step 800, loss 34.1094, acc 0.762055\n",
      "\n",
      "Train: 2017-12-10T00:50:32.663649: step 900, loss 40.4315, acc 0.666667\n",
      "Dev:   2017-12-10T00:50:34.854205: step 900, loss 34.061, acc 0.75856\n",
      "\n",
      "Train: 2017-12-10T00:50:45.469638: step 1000, loss 38.7826, acc 0.703704\n",
      "Dev:   2017-12-10T00:50:47.530498: step 1000, loss 32.953, acc 0.764151\n",
      "Saved model checkpoint to /home/nbuser/library/runs/1512866911/checkpoints/model-1001\n",
      "\n",
      "Train: 2017-12-10T00:51:00.104071: step 1100, loss 32.6943, acc 0.722222\n",
      "Dev:   2017-12-10T00:51:02.198416: step 1100, loss 31.6186, acc 0.779874\n",
      "\n",
      "Train: 2017-12-10T00:51:12.766494: step 1200, loss 27.1324, acc 0.796296\n",
      "Dev:   2017-12-10T00:51:14.702151: step 1200, loss 31.4463, acc 0.782669\n",
      "\n",
      "Train: 2017-12-10T00:51:24.856678: step 1300, loss 26.7617, acc 0.777778\n",
      "Dev:   2017-12-10T00:51:26.910623: step 1300, loss 31.8622, acc 0.780573\n",
      "\n",
      "Train: 2017-12-10T00:51:36.745011: step 1400, loss 39.9048, acc 0.740741\n",
      "Dev:   2017-12-10T00:51:38.778799: step 1400, loss 30.3261, acc 0.788609\n",
      "\n",
      "Train: 2017-12-10T00:51:48.636289: step 1500, loss 36.8671, acc 0.796296\n",
      "Dev:   2017-12-10T00:51:50.796513: step 1500, loss 30.3705, acc 0.788959\n",
      "Saved model checkpoint to /home/nbuser/library/runs/1512866911/checkpoints/model-1501\n",
      "\n",
      "Train: 2017-12-10T00:52:02.413735: step 1600, loss 22.8169, acc 0.851852\n",
      "Dev:   2017-12-10T00:52:04.396499: step 1600, loss 29.8913, acc 0.792802\n",
      "\n",
      "Train: 2017-12-10T00:52:14.078977: step 1700, loss 26.4772, acc 0.851852\n",
      "Dev:   2017-12-10T00:52:16.038252: step 1700, loss 32.0897, acc 0.77673\n",
      "\n",
      "Train: 2017-12-10T00:52:26.676392: step 1800, loss 34.3052, acc 0.759259\n",
      "Dev:   2017-12-10T00:52:28.661325: step 1800, loss 29.393, acc 0.795248\n",
      "\n",
      "Train: 2017-12-10T00:52:38.345816: step 1900, loss 21.4602, acc 0.851852\n",
      "Dev:   2017-12-10T00:52:40.506670: step 1900, loss 29.1487, acc 0.795597\n",
      "\n",
      "Train: 2017-12-10T00:52:51.104287: step 2000, loss 32.3793, acc 0.814815\n",
      "Dev:   2017-12-10T00:52:53.322635: step 2000, loss 29.9262, acc 0.789308\n",
      "Saved model checkpoint to /home/nbuser/library/runs/1512866911/checkpoints/model-2001\n",
      "\n",
      "Train: 2017-12-10T00:53:04.406231: step 2100, loss 20.5025, acc 0.907407\n",
      "Dev:   2017-12-10T00:53:06.453898: step 2100, loss 28.6275, acc 0.800489\n",
      "\n",
      "Train: 2017-12-10T00:53:16.458683: step 2200, loss 33.6631, acc 0.796296\n",
      "Dev:   2017-12-10T00:53:18.401882: step 2200, loss 28.2531, acc 0.806778\n",
      "\n",
      "Train: 2017-12-10T00:53:28.522797: step 2300, loss 24.6319, acc 0.87037\n",
      "Dev:   2017-12-10T00:53:30.534843: step 2300, loss 27.9142, acc 0.808875\n",
      "\n",
      "Train: 2017-12-10T00:53:40.537148: step 2400, loss 22.1366, acc 0.814815\n",
      "Dev:   2017-12-10T00:53:42.572724: step 2400, loss 28.2381, acc 0.810971\n",
      "\n",
      "Train: 2017-12-10T00:53:52.842082: step 2500, loss 16.8289, acc 0.925926\n",
      "Dev:   2017-12-10T00:53:54.934289: step 2500, loss 27.7155, acc 0.804333\n",
      "Saved model checkpoint to /home/nbuser/library/runs/1512866911/checkpoints/model-2501\n",
      "\n",
      "Train: 2017-12-10T00:54:06.036560: step 2600, loss 25.1777, acc 0.814815\n",
      "Dev:   2017-12-10T00:54:08.150887: step 2600, loss 28.0242, acc 0.806429\n",
      "\n",
      "Train: 2017-12-10T00:54:17.860021: step 2700, loss 23.6514, acc 0.888889\n",
      "Dev:   2017-12-10T00:54:20.120078: step 2700, loss 27.4911, acc 0.81202\n",
      "\n",
      "Train: 2017-12-10T00:54:30.709533: step 2800, loss 24.7934, acc 0.814815\n",
      "Dev:   2017-12-10T00:54:32.666022: step 2800, loss 26.9504, acc 0.820405\n",
      "\n",
      "Train: 2017-12-10T00:54:42.611146: step 2900, loss 26.693, acc 0.833333\n",
      "Dev:   2017-12-10T00:54:44.592978: step 2900, loss 27.3765, acc 0.815863\n",
      "\n",
      "Train: 2017-12-10T00:54:54.851099: step 3000, loss 22.2743, acc 0.87037\n",
      "Dev:   2017-12-10T00:54:56.932523: step 3000, loss 27.5236, acc 0.812369\n",
      "Saved model checkpoint to /home/nbuser/library/runs/1512866911/checkpoints/model-3001\n",
      "\n",
      "Train: 2017-12-10T00:55:08.165735: step 3100, loss 20.3321, acc 0.888889\n",
      "Dev:   2017-12-10T00:55:10.226090: step 3100, loss 26.0654, acc 0.827393\n",
      "\n",
      "Train: 2017-12-10T00:55:19.766600: step 3200, loss 28.3569, acc 0.833333\n",
      "Dev:   2017-12-10T00:55:22.017065: step 3200, loss 26.1868, acc 0.825996\n",
      "\n",
      "Train: 2017-12-10T00:55:32.449083: step 3300, loss 20.2199, acc 0.814815\n",
      "Dev:   2017-12-10T00:55:34.435221: step 3300, loss 25.6057, acc 0.832984\n",
      "\n",
      "Train: 2017-12-10T00:55:44.211036: step 3400, loss 21.5834, acc 0.814815\n",
      "Dev:   2017-12-10T00:55:46.137815: step 3400, loss 26.5351, acc 0.820056\n",
      "\n",
      "Train: 2017-12-10T00:55:56.546112: step 3500, loss 19.717, acc 0.87037\n",
      "Dev:   2017-12-10T00:55:58.513041: step 3500, loss 27.8469, acc 0.810622\n",
      "Saved model checkpoint to /home/nbuser/library/runs/1512866911/checkpoints/model-3501\n",
      "\n",
      "Train: 2017-12-10T00:56:09.495411: step 3600, loss 15.9501, acc 0.87037\n",
      "Dev:   2017-12-10T00:56:11.543823: step 3600, loss 25.9777, acc 0.823899\n",
      "\n",
      "Train: 2017-12-10T00:56:21.958233: step 3700, loss 37.0316, acc 0.833333\n",
      "Dev:   2017-12-10T00:56:24.104845: step 3700, loss 24.7513, acc 0.828791\n",
      "\n",
      "Train: 2017-12-10T00:56:33.946449: step 3800, loss 23.9965, acc 0.814815\n",
      "Dev:   2017-12-10T00:56:35.896712: step 3800, loss 25.3372, acc 0.834731\n",
      "\n",
      "Train: 2017-12-10T00:56:45.818644: step 3900, loss 23.6077, acc 0.888889\n",
      "Dev:   2017-12-10T00:56:47.882237: step 3900, loss 25.1279, acc 0.834032\n",
      "\n",
      "Train: 2017-12-10T00:56:58.262581: step 4000, loss 24.8181, acc 0.851852\n",
      "Dev:   2017-12-10T00:57:00.265070: step 4000, loss 24.7378, acc 0.833333\n",
      "Saved model checkpoint to /home/nbuser/library/runs/1512866911/checkpoints/model-4001\n",
      "\n",
      "Train: 2017-12-10T00:57:11.516550: step 4100, loss 15.4672, acc 0.888889\n",
      "Dev:   2017-12-10T00:57:13.490372: step 4100, loss 24.1096, acc 0.838225\n",
      "\n",
      "Train: 2017-12-10T00:57:24.094309: step 4200, loss 22.6948, acc 0.87037\n",
      "Dev:   2017-12-10T00:57:26.216378: step 4200, loss 24.3333, acc 0.836827\n",
      "\n",
      "Final Valildation Evaluation:\n",
      "Dev:   2017-12-10T00:57:36.069813: step 4280, loss 24.8094, acc 0.832285\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batches = data_utils.batch_iter(train_data, batch_size=batch_size, num_epochs=num_epochs, shuffle=True)\n",
    "train_loss, train_accuracy = run_for_epochs(batches)\n",
    "\n",
    "print(\"\\nFinal Valildation Evaluation:\")\n",
    "current_step = tf.train.global_step(sess, global_step)\n",
    "dev_loss, dev_accuracy = dev_eval(dev_data, current_step, writer=dev_summary_writer)\n",
    "#print(\"Maximum validation accuracy at step {}: {}\".format(max_accuracy_step, max_accuracy))\n",
    "print(\"\")\n",
    "\n",
    "tf_helpers.write_results(current_step, train_loss, train_accuracy, dev_loss, dev_accuracy, timestamp)\n",
    "\n",
    "\"\"\"\n",
    "for v in tf.all_variables():\n",
    "    print(v.name)\n",
    "\n",
    "input_x = graph.as_graph_element(\"input_x:0\").outputs[0]\n",
    "input_y = graph.as_graph_element(\"input_y:0\").outputs[0]\n",
    "seq_lens = graph.as_graph_element(\"seq_lens:0\").outputs[0]\n",
    "\n",
    "predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "num_correct = graph.get_operation_by_name(\"output/num_correct\").outputs[0]\n",
    "\"\"\"\n",
    "\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
